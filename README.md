# SkyModelling
This git repo contains code to:
1) Create a dataset of sky flux from BOSS data. This dataset includes each sky fiber for each plate/image in the BOSS data on nersc.
2) Create file of meta data for all sky spectra
3) Fit sky spectra and separate the airglow lines from the continuum.

## Included in this repo:
* `spframe_flux.py`: Run to collect sky flux for all BOSS data. Turns spframe electrons into flux. Generates pkl files for flux, wavelength, and raw metadata.
* `fit_spectra.py`: Takes spframe spectra and fits them using least squares. Returns the model, with the airglow lines, continuum, and residuals returned individually
* `get_meta_rich_data.py`: script that takes raw metadata files generated by spframe_flux and calculates additional meta data. output file per raw_meta file (per plate)
* `get_line_sum_file.py`: script takes spframe files and rich metadata and calcualtes the line strength of a list of lines (includes mean cont values). Output is a single file for each rich meta file (####_rich_plus.fits)
* `get_line_mean_file.py`: script that calculates the mean for each observation and each line (plate, image, camera). File saved for each rich_plus file under mean_rich. 
* `get_mega_file.py`: Takes all files in a directory (line mean_rich) and compiles them into one file.
* `MetaDataEval.ipynb`: looks at the distribution of the metadata from meta_rich.npy and serves as a way to cross check the data
* `AirglowLines`: Folder with atlas of airglow lines identified by UVES. Using files in cosby.
* `util/bitmask.py`: This code is used by spframe_flux.py to decode the pixel mask.
* `util/line_list.pkl`: A list of lines to take sum of in get_line_sum_file.py.
* `util/phot_rec.npy`: COntains the cloud data used in get_rich_meta_data.py

## Code Dependencies
The BOSS flux files are saved on nersc. This code is meant to run on nersc, or if you have downloaded some of the spFrame files from nersc on to another machine, you can point to that directory. Run the code on edison or cori.

All code is in python. You will need the following packages:
* numpy, scipy, pandas, matplotlib
* astropy
* ephem (pip install ephem)
* multiprocessing
* statsmodel (conda install statsmodels)

## How to get your own sky flux dataset
* Clone this repo to nersc
* Identify a location to save your data and modify spframe_flux.py: `SAVE_DIR` with that location
* Run `spframe_flux.py' on one node (on interactive or debug). srun -n -c 64 python spframe_flux.py. If you run out of time you can just run it again. It will figure out how many file you have to go.This will output 2 .npy files for each plate: "PLATE_calibrated_sky.npy" and "raw_meta/PLATE_raw_meta.npy"
* When complete, modify get_rich_meta_data.py: `RAW_META_DATA_DIR` with the location of your data (one above the raw_meta folder)
* Run get_rich_meta_data.py `python get_rich_meta_data.py`. This will give you a new set of files under rich_meta. Should take ~25 minutes if used 64 cores.
* Run get_line_sum_file.py. This will give you a new set of files under rich_plus with all the line strenght info. 
* Run get_line_mean_file.py. This will give you a final set of files under mean_rich that includes mean/std for all line strenght measurements.
* Run get_mega_file.py.  This will give you one large meta data file with line strengths named spframe_line_sum.fits 

## How to fit your sky spectra
* **Hold off on this - working to speed up the code. If you want, you can run it in test mode.**
* Identify a location for your fit/split data
* Modify fit_spectra.py: `SAVE_DIR` with this location and `SPECTRA_DIR` with the location of the saved data above
* Run the batch script "fitspectra1.slurm" by typing `sbatch fitspectra1.slurm`
* This will take ~x hours. You will need to run the slurm script ~x times.
